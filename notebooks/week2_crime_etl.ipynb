{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f286e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fde6fc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp dir: /Users/neethusatravada/spark_tmp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/10 13:43:05 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"crime_hotspot\")\n",
    "    .config(\"spark.local.dir\", \"/Users/neethusatravada/spark_tmp\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Temp dir:\", spark.conf.get(\"spark.local.dir\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbb698fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV exists: True\n",
      "Parquet out: /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/outputs/clean_crimes_parquet\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from pathlib import Path\n",
    "\n",
    "# ✅ Paths customized for your project\n",
    "CRIMES_CSV  = Path(\"/Users/neethusatravada/Documents/DATABASE/crime_prediction_project/data/raw/chicago_crimes_2001_present.csv\")\n",
    "OUT_PARQUET = Path(\"/Users/neethusatravada/Documents/DATABASE/crime_prediction_project/outputs/clean_crimes_parquet\")\n",
    "\n",
    "OUT_PARQUET.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"CSV exists:\", CRIMES_CSV.exists())\n",
    "print(\"Parquet out:\", OUT_PARQUET.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af912a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:==============================>                           (8 + 7) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw row count: 8435617\n",
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Case Number: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Block: string (nullable = true)\n",
      " |-- IUCR: string (nullable = true)\n",
      " |-- Primary Type: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Location Description: string (nullable = true)\n",
      " |-- Arrest: boolean (nullable = true)\n",
      " |-- Domestic: boolean (nullable = true)\n",
      " |-- Beat: integer (nullable = true)\n",
      " |-- District: integer (nullable = true)\n",
      " |-- Ward: integer (nullable = true)\n",
      " |-- Community Area: integer (nullable = true)\n",
      " |-- FBI Code: string (nullable = true)\n",
      " |-- X Coordinate: integer (nullable = true)\n",
      " |-- Y Coordinate: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Updated On: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      "\n",
      "+--------+-----------+----------------------+-----------------------+----+--------------------------+------------------------------+--------------------------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+----------------------+------------+-------------+-----------------------------+\n",
      "|ID      |Case Number|Date                  |Block                  |IUCR|Primary Type              |Description                   |Location Description                  |Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|Updated On            |Latitude    |Longitude    |Location                     |\n",
      "+--------+-----------+----------------------+-----------------------+----+--------------------------+------------------------------+--------------------------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+----------------------+------------+-------------+-----------------------------+\n",
      "|13311263|JG503434   |07/29/2022 03:39:00 AM|023XX S TROY ST        |1582|OFFENSE INVOLVING CHILDREN|CHILD PORNOGRAPHY             |RESIDENCE                             |true  |false   |1033|10      |25  |30            |17      |NULL        |NULL        |2022|04/18/2024 03:40:59 PM|NULL        |NULL         |NULL                         |\n",
      "|13053066|JG103252   |01/03/2023 04:44:00 PM|039XX W WASHINGTON BLVD|2017|NARCOTICS                 |MANUFACTURE / DELIVER - CRACK |SIDEWALK                              |true  |false   |1122|11      |28  |26            |18      |NULL        |NULL        |2023|01/20/2024 03:41:12 PM|NULL        |NULL         |NULL                         |\n",
      "|12131221|JD327000   |08/10/2020 09:45:00 AM|015XX N DAMEN AVE      |0326|ROBBERY                   |AGGRAVATED VEHICULAR HIJACKING|STREET                                |true  |false   |1424|14      |1   |24            |03      |1162795     |1909900     |2020|05/17/2025 03:40:52 PM|41.908417822|-87.67740693 |(41.908417822, -87.67740693) |\n",
      "|11227634|JB147599   |08/26/2017 10:00:00 AM|001XX W RANDOLPH ST    |0281|CRIM SEXUAL ASSAULT       |NON-AGGRAVATED                |HOTEL/MOTEL                           |false |false   |122 |1       |42  |32            |02      |NULL        |NULL        |2017|02/11/2018 03:57:41 PM|NULL        |NULL         |NULL                         |\n",
      "|13203321|JG415333   |09/06/2023 05:00:00 PM|002XX N Wells st       |1320|CRIMINAL DAMAGE           |TO VEHICLE                    |PARKING LOT / GARAGE (NON RESIDENTIAL)|false |false   |122 |1       |42  |32            |14      |1174694     |1901831     |2023|11/04/2023 03:40:18 PM|41.886018055|-87.633937881|(41.886018055, -87.633937881)|\n",
      "+--------+-----------+----------------------+-----------------------+----+--------------------------+------------------------------+--------------------------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+----------------------+------------+-------------+-----------------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Read the full dataset into Spark\n",
    "crimes_raw = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(str(CRIMES_CSV))\n",
    ")\n",
    "\n",
    "print(\"Raw row count:\", crimes_raw.count())\n",
    "crimes_raw.printSchema()\n",
    "crimes_raw.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb027a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-------------------+\n",
      "|date                  |date_ts            |\n",
      "+----------------------+-------------------+\n",
      "|07/29/2022 03:39:00 AM|2022-07-29 03:39:00|\n",
      "|01/03/2023 04:44:00 PM|2023-01-03 16:44:00|\n",
      "|08/10/2020 09:45:00 AM|2020-08-10 09:45:00|\n",
      "|08/26/2017 10:00:00 AM|2017-08-26 10:00:00|\n",
      "|09/06/2023 05:00:00 PM|2023-09-06 17:00:00|\n",
      "|09/06/2023 11:00:00 AM|2023-09-06 11:00:00|\n",
      "|05/21/2019 08:20:00 AM|2019-05-21 08:20:00|\n",
      "|07/07/2021 10:30:00 AM|2021-07-07 10:30:00|\n",
      "|06/14/2022 02:47:00 PM|2022-06-14 14:47:00|\n",
      "|09/21/2022 10:00:00 PM|2022-09-21 22:00:00|\n",
      "+----------------------+-------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "def to_snake(name: str) -> str:\n",
    "    return (\n",
    "        name.lower()\n",
    "        .replace(\" \", \"_\")\n",
    "        .replace(\"/\", \"_\")\n",
    "        .replace(\"-\", \"_\")\n",
    "        .replace(\"(\", \"\")\n",
    "        .replace(\")\", \"\")\n",
    "    )\n",
    "\n",
    "df = crimes_raw\n",
    "for c in df.columns:\n",
    "    df = df.withColumnRenamed(c, to_snake(c))\n",
    "\n",
    "# Convert date to timestamp (correct Chicago format)\n",
    "df = df.withColumn(\"date_ts\", F.to_timestamp(\"date\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "\n",
    "# Quick sanity check\n",
    "df.select(\"date\", \"date_ts\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff242bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=============================================>           (20 + 5) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned row count: 8341471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Drop duplicates\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "# Remove rows with nulls in key fields\n",
    "critical_cols = [\"primary_type\", \"date_ts\", \"latitude\", \"longitude\"]\n",
    "for col in critical_cols:\n",
    "    df = df.filter(F.col(col).isNotNull())\n",
    "\n",
    "# Chicago bounding box filter\n",
    "df = df.filter(\n",
    "    (F.col(\"latitude\").between(41.60, 42.10)) &\n",
    "    (F.col(\"longitude\").between(-87.95, -87.50))\n",
    ")\n",
    "\n",
    "# Remove future-dated records\n",
    "df = df.filter(F.col(\"date_ts\") <= F.current_timestamp())\n",
    "\n",
    "print(\"Cleaned row count:\", df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c88a1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----+----+---------+------+---------+---------+\n",
      "|date_ts            |year|month|hour|dayofweek|season|lat_round|lon_round|\n",
      "+-------------------+----+-----+----+---------+------+---------+---------+\n",
      "|2023-09-06 21:55:00|2023|9    |21  |Wed      |fall  |41.917   |-87.714  |\n",
      "|2023-09-06 13:58:00|2023|9    |13  |Wed      |fall  |41.745   |-87.597  |\n",
      "|2023-10-08 00:01:00|2023|10   |0   |Sun      |fall  |41.973   |-87.679  |\n",
      "|2023-09-07 18:00:00|2023|9    |18  |Thu      |fall  |41.95    |-87.746  |\n",
      "|2023-09-07 03:45:00|2023|9    |3   |Thu      |fall  |41.778   |-87.78   |\n",
      "+-------------------+----+-----+----+---------+------+---------+---------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "df = (\n",
    "    df\n",
    "    .withColumn(\"year\", F.year(\"date_ts\"))\n",
    "    .withColumn(\"month\", F.month(\"date_ts\"))\n",
    "    .withColumn(\"hour\", F.hour(\"date_ts\"))\n",
    "    .withColumn(\"dayofweek\", F.date_format(\"date_ts\", \"E\"))\n",
    "    .withColumn(\n",
    "        \"season\",\n",
    "        F.when(F.col(\"month\").isin(12, 1, 2), \"winter\")\n",
    "         .when(F.col(\"month\").isin(3, 4, 5), \"spring\")\n",
    "         .when(F.col(\"month\").isin(6, 7, 8), \"summer\")\n",
    "         .otherwise(\"fall\")\n",
    "    )\n",
    "    .withColumn(\"lat_round\", F.round(\"latitude\", 3))\n",
    "    .withColumn(\"lon_round\", F.round(\"longitude\", 3))\n",
    ")\n",
    "\n",
    "df.select(\"date_ts\",\"year\",\"month\",\"hour\",\"dayofweek\",\"season\",\"lat_round\",\"lon_round\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16a60ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/10 13:44:57 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+-----+----+------------+-----------+--------------------+------+--------+----+--------+------+--------------+--------+------------+------------+----+----------+--------+---------+--------+-------+-----+----+---------+------+---------+---------+\n",
      "| id|case_number|date|block|iucr|primary_type|description|location_description|arrest|domestic|beat|district|  ward|community_area|fbi_code|x_coordinate|y_coordinate|year|updated_on|latitude|longitude|location|date_ts|month|hour|dayofweek|season|lat_round|lon_round|\n",
      "+---+-----------+----+-----+----+------------+-----------+--------------------+------+--------+----+--------+------+--------------+--------+------------+------------+----+----------+--------+---------+--------+-------+-----+----+---------+------+---------+---------+\n",
      "|  0|          0|   0|    0|   0|           0|          0|                9820|     0|       0|   0|      47|605532|        604445|       0|           0|           0|   0|         0|       0|        0|       0|      0|    0|   0|        0|     0|        0|        0|\n",
      "+---+-----------+----+-----+----+------------+-----------+--------------------+------+--------+----+--------+------+--------------+--------+------------+------------+----+----------+--------+---------+--------+-------+-----+----+---------+------+---------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/10 13:53:21 ERROR Executor: Exception in task 10.0 in stage 25.0 (TID 158)\n",
      "org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text 'false' could not be parsed at index 0. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:279)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/11/10 13:53:21 ERROR Executor: Exception in task 9.0 in stage 25.0 (TID 157)\n",
      "org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text 'AGGRAVATED: OTHER DANG WEAPON' could not be parsed at index 0. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:279)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/11/10 13:53:21 ERROR Executor: Exception in task 11.0 in stage 25.0 (TID 159)\n",
      "org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text 'false' could not be parsed at index 0. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:279)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/11/10 13:53:21 WARN TaskSetManager: Lost task 9.0 in stage 25.0 (TID 157) (10-20-4-140.dynapool.wireless.nyu.edu executor driver): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text 'AGGRAVATED: OTHER DANG WEAPON' could not be parsed at index 0. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:279)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "25/11/10 13:53:21 ERROR TaskSetManager: Task 9 in stage 25.0 failed 1 times; aborting job\n",
      "25/11/10 13:53:21 WARN TaskSetManager: Lost task 11.0 in stage 25.0 (TID 159) (10-20-4-140.dynapool.wireless.nyu.edu executor driver): org.apache.spark.SparkDateTimeException: [CANNOT_PARSE_TIMESTAMP] Text 'false' could not be parsed at index 0. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.ansiDateTimeParseError(QueryExecutionErrors.scala:279)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.ansiDateTimeParseError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "25/11/10 13:53:21 WARN TaskSetManager: Lost task 12.0 in stage 25.0 (TID 160) (10-20-4-140.dynapool.wireless.nyu.edu executor driver): TaskKilled (Stage cancelled: [CANNOT_PARSE_TIMESTAMP] Text 'AGGRAVATED: OTHER DANG WEAPON' could not be parsed at index 0. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007)\n",
      "25/11/10 13:53:21 WARN TaskSetManager: Lost task 13.0 in stage 25.0 (TID 161) (10-20-4-140.dynapool.wireless.nyu.edu executor driver): TaskKilled (Stage cancelled: [CANNOT_PARSE_TIMESTAMP] Text 'AGGRAVATED: OTHER DANG WEAPON' could not be parsed at index 0. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007)\n",
      "25/11/10 13:53:21 WARN TaskSetManager: Lost task 14.0 in stage 25.0 (TID 162) (10-20-4-140.dynapool.wireless.nyu.edu executor driver): TaskKilled (Stage cancelled: [CANNOT_PARSE_TIMESTAMP] Text 'AGGRAVATED: OTHER DANG WEAPON' could not be parsed at index 0. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007)\n"
     ]
    },
    {
     "ename": "DateTimeException",
     "evalue": "[CANNOT_PARSE_TIMESTAMP] Text 'AGGRAVATED: OTHER DANG WEAPON' could not be parsed at index 0. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDateTimeException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal records: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDistinct records: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdistinct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Handle missing values in important columns\u001b[39;00m\n\u001b[32m      9\u001b[39m df = df.filter(F.col(\u001b[33m\"\u001b[39m\u001b[33mlatitude\u001b[39m\u001b[33m\"\u001b[39m).isNotNull() & F.col(\u001b[33m\"\u001b[39m\u001b[33mlongitude\u001b[39m\u001b[33m\"\u001b[39m).isNotNull())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages/pyspark/sql/classic/dataframe.py:439\u001b[39m, in \u001b[36mDataFrame.count\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mDateTimeException\u001b[39m: [CANNOT_PARSE_TIMESTAMP] Text 'AGGRAVATED: OTHER DANG WEAPON' could not be parsed at index 0. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"Total records: {df.count()}\")\n",
    "print(f\"Distinct records: {df.distinct().count()}\")\n",
    "\n",
    "# Handle missing values in important columns\n",
    "df = df.filter(F.col(\"latitude\").isNotNull() & F.col(\"longitude\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30c1a373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|       primary_type| count|\n",
      "+-------------------+------+\n",
      "|              THEFT|943844|\n",
      "|            BATTERY|862639|\n",
      "|    CRIMINAL DAMAGE|554710|\n",
      "|          NARCOTICS|497065|\n",
      "|            ASSAULT|288185|\n",
      "|           BURGLARY|271516|\n",
      "|MOTOR VEHICLE THEFT|232175|\n",
      "|      OTHER OFFENSE|194431|\n",
      "|            ROBBERY|175361|\n",
      "| DECEPTIVE PRACTICE|147082|\n",
      "+-------------------+------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|hour| count|\n",
      "+----+------+\n",
      "|   0|473837|\n",
      "|   1|266800|\n",
      "|   2|225792|\n",
      "|   3|184344|\n",
      "|   4|140776|\n",
      "|   5|118057|\n",
      "|   6|135858|\n",
      "|   7|192540|\n",
      "|   8|281635|\n",
      "|   9|355451|\n",
      "|  10|352843|\n",
      "|  11|369480|\n",
      "|  12|475539|\n",
      "|  13|394681|\n",
      "|  14|419171|\n",
      "|  15|445241|\n",
      "|  16|424030|\n",
      "|  17|431276|\n",
      "|  18|456303|\n",
      "|  19|468608|\n",
      "+----+------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|season|  count|\n",
      "+------+-------+\n",
      "|winter|1839700|\n",
      "|summer|2293989|\n",
      "|spring|2108058|\n",
      "|  fall|2099724|\n",
      "+------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 84:===============================================>        (21 + 4) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dayofweek|  count|\n",
      "+---------+-------+\n",
      "|      Sun|1140742|\n",
      "|      Mon|1180580|\n",
      "|      Thu|1185506|\n",
      "|      Sat|1197557|\n",
      "|      Wed|1197134|\n",
      "|      Fri|1251353|\n",
      "|      Tue|1188599|\n",
      "+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Crime counts by type\n",
    "df.groupBy(\"primary_type\").count().orderBy(F.desc(\"count\")).show(10)\n",
    "\n",
    "# Crimes by hour\n",
    "df.groupBy(\"hour\").count().orderBy(\"hour\").show()\n",
    "\n",
    "# Crimes by season\n",
    "df.groupBy(\"season\").count().show()\n",
    "\n",
    "# Crimes by day of week\n",
    "df.groupBy(\"dayofweek\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d081c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c0e05ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MISSING VALUES CHECK\n",
      "======================================================================\n",
      "\n",
      "Missing values in NUMERIC columns:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+------+--------------+------------+------------+----+--------+---------+-----+----+---------+---------+\n",
      "| id|beat|district|  ward|community_area|x_coordinate|y_coordinate|year|latitude|longitude|month|hour|lat_round|lon_round|\n",
      "+---+----+--------+------+--------------+------------+------------+----+--------+---------+-----+----+---------+---------+\n",
      "|  0|   0|      47|605532|        604445|           0|           0|   0|       0|        0|    0|   0|        0|        0|\n",
      "+---+----+--------+------+--------------+------------+------------+----+--------+---------+-----+----+---------+---------+\n",
      "\n",
      "\n",
      "Missing values in NON-NUMERIC columns:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 70:=====================================================>  (24 + 1) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-----+----+------------+-----------+--------------------+------+--------+--------+----------+--------+-------+---------+------+\n",
      "|case_number|date|block|iucr|primary_type|description|location_description|arrest|domestic|fbi_code|updated_on|location|date_ts|dayofweek|season|\n",
      "+-----------+----+-----+----+------------+-----------+--------------------+------+--------+--------+----------+--------+-------+---------+------+\n",
      "|          0|   0|    0|   0|           0|          0|                9820|     0|       0|       0|         0|       0|      0|        0|     0|\n",
      "+-----------+----+-----+----+------------+-----------+--------------------+------+--------+--------+----------+--------+-------+---------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check for missing values (handles both numeric and string columns)\n",
    "from pyspark.sql.functions import col, count, when, isnan\n",
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MISSING VALUES CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get numeric and non-numeric columns\n",
    "numeric_cols = [c for c, dtype in df.dtypes if dtype in ['double', 'float', 'int', 'bigint']]\n",
    "non_numeric_cols = [c for c in df.columns if c not in numeric_cols]\n",
    "\n",
    "# Check missing values for numeric columns (use isnan)\n",
    "if numeric_cols:\n",
    "    missing_numeric = df.select([\n",
    "        count(when(col(c).isNull() | isnan(c), c)).alias(c) \n",
    "        for c in numeric_cols\n",
    "    ])\n",
    "    print(\"\\nMissing values in NUMERIC columns:\")\n",
    "    missing_numeric.show()\n",
    "\n",
    "# Check missing values for non-numeric columns (only use isNull)\n",
    "if non_numeric_cols:\n",
    "    missing_non_numeric = df.select([\n",
    "        count(when(col(c).isNull(), c)).alias(c) \n",
    "        for c in non_numeric_cols\n",
    "    ])\n",
    "    print(\"\\nMissing values in NON-NUMERIC columns:\")\n",
    "    missing_non_numeric.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "615ffeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DOWNLOADING CHICAGO WEATHER DATA\n",
      "======================================================================\n",
      "\n",
      "1. Fetching weather data from Open-Meteo API...\n",
      "   (This may take a few minutes...)\n",
      "   ✓ Downloaded 8,766 days of weather data\n",
      "   ✓ Saved to: ../data/raw/chicago_weather_2001_2024.csv\n",
      "   ✓ Date range: 2001-01-01 00:00:00 to 2024-12-31 00:00:00\n",
      "\n",
      "   Sample weather data:\n",
      "        date  temp_max  temp_min  temp_mean  precipitation  wind_speed\n",
      "0 2001-01-01      22.2      12.4       17.1          0.000        11.5\n",
      "1 2001-01-02      15.1       4.2        9.8          0.000        15.3\n",
      "2 2001-01-03      26.4       9.6       19.2          0.008        16.9\n",
      "3 2001-01-04      30.9      16.5       23.6          0.000        22.3\n",
      "4 2001-01-05      34.0      25.0       31.1          0.004        17.5\n"
     ]
    }
   ],
   "source": [
    "# Download Chicago weather data\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DOWNLOADING CHICAGO WEATHER DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Option 1: Download from NOAA Climate Data Online\n",
    "# We'll use a pre-processed dataset from a reliable source\n",
    "\n",
    "weather_url = \"https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/\"\n",
    "\n",
    "# For simplicity, let's use a different approach - download historical weather from Open-Meteo\n",
    "# This is a free weather API with historical data\n",
    "\n",
    "print(\"\\n1. Fetching weather data from Open-Meteo API...\")\n",
    "print(\"   (This may take a few minutes...)\")\n",
    "\n",
    "# We'll get daily weather data for Chicago (2001-2024)\n",
    "# Chicago coordinates: 41.8781, -87.6298\n",
    "\n",
    "base_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "# Define date range\n",
    "start_date = \"2001-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "\n",
    "params = {\n",
    "    \"latitude\": 41.8781,\n",
    "    \"longitude\": -87.6298,\n",
    "    \"start_date\": start_date,\n",
    "    \"end_date\": end_date,\n",
    "    \"daily\": \"temperature_2m_max,temperature_2m_min,temperature_2m_mean,precipitation_sum,windspeed_10m_max\",\n",
    "    \"temperature_unit\": \"fahrenheit\",\n",
    "    \"windspeed_unit\": \"mph\",\n",
    "    \"precipitation_unit\": \"inch\",\n",
    "    \"timezone\": \"America/Chicago\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.get(base_url, params=params, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    weather_data = response.json()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    weather_df = pd.DataFrame({\n",
    "        'date': weather_data['daily']['time'],\n",
    "        'temp_max': weather_data['daily']['temperature_2m_max'],\n",
    "        'temp_min': weather_data['daily']['temperature_2m_min'],\n",
    "        'temp_mean': weather_data['daily']['temperature_2m_mean'],\n",
    "        'precipitation': weather_data['daily']['precipitation_sum'],\n",
    "        'wind_speed': weather_data['daily']['windspeed_10m_max']\n",
    "    })\n",
    "    \n",
    "    # Convert date to datetime\n",
    "    weather_df['date'] = pd.to_datetime(weather_df['date'])\n",
    "    \n",
    "    # Save to CSV\n",
    "    weather_path = '../data/raw/chicago_weather_2001_2024.csv'\n",
    "    weather_df.to_csv(weather_path, index=False)\n",
    "    \n",
    "    print(f\"   ✓ Downloaded {len(weather_df):,} days of weather data\")\n",
    "    print(f\"   ✓ Saved to: {weather_path}\")\n",
    "    print(f\"   ✓ Date range: {weather_df['date'].min()} to {weather_df['date'].max()}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\n   Sample weather data:\")\n",
    "    print(weather_df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ✗ Error downloading weather data: {e}\")\n",
    "    print(\"   → We'll create a backup method...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d95eac21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DOWNLOADING CHICAGO CENSUS/DEMOGRAPHIC DATA\n",
      "======================================================================\n",
      "\n",
      "2. Fetching census data from Chicago Data Portal...\n",
      "   ✓ Downloaded census data for 78 community areas\n",
      "   ✓ Saved to: ../data/raw/chicago_census_community_areas.csv\n",
      "\n",
      "   Sample census data:\n",
      "  ca community_area_name percent_of_housing_crowded  \\\n",
      "0  1         Rogers Park                        7.7   \n",
      "1  2          West Ridge                        7.8   \n",
      "2  3              Uptown                        3.8   \n",
      "3  4      Lincoln Square                        3.4   \n",
      "4  5        North Center                        0.3   \n",
      "\n",
      "  percent_households_below_poverty percent_aged_16_unemployed  \\\n",
      "0                             23.6                        8.7   \n",
      "1                             17.2                        8.8   \n",
      "2                               24                        8.9   \n",
      "3                             10.9                        8.2   \n",
      "4                              7.5                        5.2   \n",
      "\n",
      "  percent_aged_25_without_high_school_diploma  \\\n",
      "0                                        18.2   \n",
      "1                                        20.8   \n",
      "2                                        11.8   \n",
      "3                                        13.4   \n",
      "4                                         4.5   \n",
      "\n",
      "  percent_aged_under_18_or_over_64 per_capita_income_ hardship_index  \n",
      "0                             27.5              23939             39  \n",
      "1                             38.5              23040             46  \n",
      "2                             22.2              35787             20  \n",
      "3                             25.5              37524             17  \n",
      "4                             26.2              57123              6  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DOWNLOADING CHICAGO CENSUS/DEMOGRAPHIC DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n2. Fetching census data from Chicago Data Portal...\")\n",
    "\n",
    "# Chicago community areas demographic data\n",
    "census_url = \"https://data.cityofchicago.org/resource/kn9c-c2s2.json\"\n",
    "\n",
    "try:\n",
    "    # Get census data via API\n",
    "    response = requests.get(census_url, params={\"$limit\": 100}, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    census_data = response.json()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    census_df = pd.DataFrame(census_data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    census_path = '../data/raw/chicago_census_community_areas.csv'\n",
    "    census_df.to_csv(census_path, index=False)\n",
    "    \n",
    "    print(f\"   ✓ Downloaded census data for {len(census_df)} community areas\")\n",
    "    print(f\"   ✓ Saved to: {census_path}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\n   Sample census data:\")\n",
    "    print(census_df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ✗ Error downloading census data: {e}\")\n",
    "    print(\"   → Will try alternative source...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bb5c9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DOWNLOADING CHICAGO CENSUS/DEMOGRAPHIC DATA\n",
      "======================================================================\n",
      "\n",
      "2. Fetching census data from Chicago Data Portal...\n",
      "   ✓ Downloaded census data for 78 community areas\n",
      "   ✓ Saved to: ../data/raw/chicago_census_community_areas.csv\n",
      "\n",
      "   Sample census data:\n",
      "  ca community_area_name percent_of_housing_crowded  \\\n",
      "0  1         Rogers Park                        7.7   \n",
      "1  2          West Ridge                        7.8   \n",
      "2  3              Uptown                        3.8   \n",
      "3  4      Lincoln Square                        3.4   \n",
      "4  5        North Center                        0.3   \n",
      "\n",
      "  percent_households_below_poverty percent_aged_16_unemployed  \\\n",
      "0                             23.6                        8.7   \n",
      "1                             17.2                        8.8   \n",
      "2                               24                        8.9   \n",
      "3                             10.9                        8.2   \n",
      "4                              7.5                        5.2   \n",
      "\n",
      "  percent_aged_25_without_high_school_diploma  \\\n",
      "0                                        18.2   \n",
      "1                                        20.8   \n",
      "2                                        11.8   \n",
      "3                                        13.4   \n",
      "4                                         4.5   \n",
      "\n",
      "  percent_aged_under_18_or_over_64 per_capita_income_ hardship_index  \n",
      "0                             27.5              23939             39  \n",
      "1                             38.5              23040             46  \n",
      "2                             22.2              35787             20  \n",
      "3                             25.5              37524             17  \n",
      "4                             26.2              57123              6  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DOWNLOADING CHICAGO CENSUS/DEMOGRAPHIC DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n2. Fetching census data from Chicago Data Portal...\")\n",
    "\n",
    "# Chicago community areas demographic data\n",
    "census_url = \"https://data.cityofchicago.org/resource/kn9c-c2s2.json\"\n",
    "\n",
    "try:\n",
    "    # Get census data via API\n",
    "    response = requests.get(census_url, params={\"$limit\": 100}, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    census_data = response.json()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    census_df = pd.DataFrame(census_data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    census_path = '../data/raw/chicago_census_community_areas.csv'\n",
    "    census_df.to_csv(census_path, index=False)\n",
    "    \n",
    "    print(f\"   ✓ Downloaded census data for {len(census_df)} community areas\")\n",
    "    print(f\"   ✓ Saved to: {census_path}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\n   Sample census data:\")\n",
    "    print(census_df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ✗ Error downloading census data: {e}\")\n",
    "    print(\"   → Will try alternative source...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7045020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA CLEANING & QUALITY CHECKS\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Total Records: 8,341,471\n",
      "\n",
      "2. Duplicate Check:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Distinct records: 8,341,471\n",
      "   Duplicate records: 0\n",
      "\n",
      "3. Cleaning Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Records after cleaning: 8,341,471\n",
      "   Records removed: 0 (0.00%)\n",
      "\n",
      "4. Coordinate Validity Check:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Valid coordinates: 8,313,956 (99.67%)\n",
      "   Invalid coordinates: 27,515 (0.33%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 102:>                                                       (0 + 8) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3251.459s][warning][gc,alloc] Executor task launch worker for task 7.0 in stage 102.0 (TID 689): Retried waiting for GCLocker too often allocating 131074 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/10 11:54:48 ERROR Executor: Exception in task 7.0 in stage 102.0 (TID 689)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:54)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:559)\n",
      "\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.sortedIterator(UnsafeKVExternalSorter.java:206)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.finishAggregate(HashAggregateExec.scala:226)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3055/0x000000e001ec71e0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/11/10 11:54:48 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 7.0 in stage 102.0 (TID 689),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:54)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:559)\n",
      "\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.sortedIterator(UnsafeKVExternalSorter.java:206)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.finishAggregate(HashAggregateExec.scala:226)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3055/0x000000e001ec71e0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/11/10 11:54:48 WARN TaskSetManager: Lost task 7.0 in stage 102.0 (TID 689) (10-20-4-140.dynapool.wireless.nyu.edu executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:54)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:559)\n",
      "\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.sortedIterator(UnsafeKVExternalSorter.java:206)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.finishAggregate(HashAggregateExec.scala:226)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3055/0x000000e001ec71e0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "25/11/10 11:54:48 ERROR TaskSetManager: Task 7 in stage 102.0 failed 1 times; aborting job\n",
      "25/11/10 11:54:48 WARN TaskSetManager: Lost task 6.0 in stage 102.0 (TID 688) (10-20-4-140.dynapool.wireless.nyu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 7 in stage 102.0 failed 1 times, most recent failure: Lost task 7.0 in stage 102.0 (TID 689) (10-20-4-140.dynapool.wireless.nyu.edu executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:54)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:559)\n",
      "\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.sortedIterator(UnsafeKVExternalSorter.java:206)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.finishAggregate(HashAggregateExec.scala:226)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3055/0x000000e001ec71e0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1187.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 102.0 failed 1 times, most recent failure: Lost task 7.0 in stage 102.0 (TID 689) (10-20-4-140.dynapool.wireless.nyu.edu executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:54)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:559)\n\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.sortedIterator(UnsafeKVExternalSorter.java:206)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.finishAggregate(HashAggregateExec.scala:226)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3055/0x000000e001ec71e0.apply(Unknown Source)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:54)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:559)\n\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.sortedIterator(UnsafeKVExternalSorter.java:206)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.finishAggregate(HashAggregateExec.scala:226)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3055/0x000000e001ec71e0.apply(Unknown Source)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# 5. Filter to valid coordinates only\u001b[39;00m\n\u001b[32m     43\u001b[39m df_clean = df_clean.filter(\n\u001b[32m     44\u001b[39m     (col(\u001b[33m\"\u001b[39m\u001b[33mlatitude\u001b[39m\u001b[33m\"\u001b[39m) >= \u001b[32m41.6\u001b[39m) & (col(\u001b[33m\"\u001b[39m\u001b[33mlatitude\u001b[39m\u001b[33m\"\u001b[39m) <= \u001b[32m42.1\u001b[39m) &\n\u001b[32m     45\u001b[39m     (col(\u001b[33m\"\u001b[39m\u001b[33mlongitude\u001b[39m\u001b[33m\"\u001b[39m) >= -\u001b[32m87.9\u001b[39m) & (col(\u001b[33m\"\u001b[39m\u001b[33mlongitude\u001b[39m\u001b[33m\"\u001b[39m) <= -\u001b[32m87.5\u001b[39m)\n\u001b[32m     46\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m final_count = \u001b[43mdf_clean\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m5. Final Clean Dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_count\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m records\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# 6. Cache the clean dataset for faster processing\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages/pyspark/sql/classic/dataframe.py:439\u001b[39m, in \u001b[36mDataFrame.count\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o1187.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 102.0 failed 1 times, most recent failure: Lost task 7.0 in stage 102.0 (TID 689) (10-20-4-140.dynapool.wireless.nyu.edu executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:54)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:559)\n\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.sortedIterator(UnsafeKVExternalSorter.java:206)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.finishAggregate(HashAggregateExec.scala:226)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3055/0x000000e001ec71e0.apply(Unknown Source)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:54)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:559)\n\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.sortedIterator(UnsafeKVExternalSorter.java:206)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.finishAggregate(HashAggregateExec.scala:226)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3055/0x000000e001ec71e0.apply(Unknown Source)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "# Continue with the rest of cleaning\n",
    "print(\"=\"*70)\n",
    "print(\"DATA CLEANING & QUALITY CHECKS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Total records\n",
    "total_records = df.count()\n",
    "print(f\"\\n1. Total Records: {total_records:,}\")\n",
    "\n",
    "# 2. Check for duplicates\n",
    "print(\"\\n2. Duplicate Check:\")\n",
    "distinct_records = df.distinct().count()\n",
    "duplicates = total_records - distinct_records\n",
    "print(f\"   Distinct records: {distinct_records:,}\")\n",
    "print(f\"   Duplicate records: {duplicates:,}\")\n",
    "\n",
    "# 3. Remove records with missing critical data (coordinates and date)\n",
    "print(\"\\n3. Cleaning Data...\")\n",
    "df_clean = df.filter(\n",
    "    col(\"latitude\").isNotNull() & \n",
    "    col(\"longitude\").isNotNull() &\n",
    "    col(\"date_ts\").isNotNull() &\n",
    "    col(\"primary_type\").isNotNull()\n",
    ")\n",
    "\n",
    "cleaned_count = df_clean.count()\n",
    "removed = total_records - cleaned_count\n",
    "print(f\"   Records after cleaning: {cleaned_count:,}\")\n",
    "print(f\"   Records removed: {removed:,} ({(removed/total_records)*100:.2f}%)\")\n",
    "\n",
    "# 4. Check coordinate validity (Chicago boundaries)\n",
    "print(\"\\n4. Coordinate Validity Check:\")\n",
    "valid_coords = df_clean.filter(\n",
    "    (col(\"latitude\") >= 41.6) & (col(\"latitude\") <= 42.1) &  # Chicago lat range\n",
    "    (col(\"longitude\") >= -87.9) & (col(\"longitude\") <= -87.5)  # Chicago lon range\n",
    ").count()\n",
    "\n",
    "invalid_coords = cleaned_count - valid_coords\n",
    "print(f\"   Valid coordinates: {valid_coords:,} ({(valid_coords/cleaned_count)*100:.2f}%)\")\n",
    "print(f\"   Invalid coordinates: {invalid_coords:,} ({(invalid_coords/cleaned_count)*100:.2f}%)\")\n",
    "\n",
    "# 5. Filter to valid coordinates only\n",
    "df_clean = df_clean.filter(\n",
    "    (col(\"latitude\") >= 41.6) & (col(\"latitude\") <= 42.1) &\n",
    "    (col(\"longitude\") >= -87.9) & (col(\"longitude\") <= -87.5)\n",
    ")\n",
    "\n",
    "final_count = df_clean.count()\n",
    "print(f\"\\n5. Final Clean Dataset: {final_count:,} records\")\n",
    "\n",
    "# 6. Cache the clean dataset for faster processing\n",
    "df_clean.cache()\n",
    "print(f\"   ✓ Dataset cached in memory\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ DATA CLEANING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show sample of clean data\n",
    "print(\"\\nSample of clean data:\")\n",
    "df_clean.select(\"date_ts\", \"primary_type\", \"latitude\", \"longitude\", \"year\", \"season\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e29a50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/10 13:38:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark restarted with 8GB memory\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Restart Spark with MUCH more memory\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CrimePrediction\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✓ Spark restarted with 8GB memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9fe3dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING DATA (WITH SAMPLING FOR MEMORY EFFICIENCY)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/10 13:40:14 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: ../data/raw/Crimes_-_2001_to_Present.csv.\n",
      "java.io.FileNotFoundException: File ../data/raw/Crimes_-_2001_to_Present.csv does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:392)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:259)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/Users/neethusatravada/Documents/DATABASE/crime_prediction_project/data/raw/Crimes_-_2001_to_Present.csv. SQLSTATE: 42K03",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Load ONLY 2 million records to avoid memory issues\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/raw/Crimes_-_2001_to_Present.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m.limit(\u001b[32m2000000\u001b[39m)  \u001b[38;5;66;03m# Load only 2M records\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.count()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m records\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Convert date\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages/pyspark/sql/readwriter.py:838\u001b[39m, in \u001b[36mDataFrameReader.csv\u001b[39m\u001b[34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[39m\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) == \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    837\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._spark._sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m838\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonUtils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_remote_only():\n\u001b[32m    841\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrdd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RDD  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [PATH_NOT_FOUND] Path does not exist: file:/Users/neethusatravada/Documents/DATABASE/crime_prediction_project/data/raw/Crimes_-_2001_to_Present.csv. SQLSTATE: 42K03"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load DATA WITH SAMPLING from the start\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING DATA (WITH SAMPLING FOR MEMORY EFFICIENCY)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load ONLY 2 million records to avoid memory issues\n",
    "df = spark.read.csv(\n",
    "    \"../data/raw/Crimes_-_2001_to_Present.csv\", \n",
    "    header=True, \n",
    "    inferSchema=True\n",
    ").limit(2000000)  # Load only 2M records\n",
    "\n",
    "print(f\"✓ Loaded {df.count():,} records\")\n",
    "\n",
    "# Convert date\n",
    "df = df.withColumn(\"date_ts\", F.to_timestamp(\"Date\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "\n",
    "# Add features\n",
    "df = (\n",
    "    df\n",
    "    .withColumn(\"year\", F.year(\"date_ts\"))\n",
    "    .withColumn(\"month\", F.month(\"date_ts\"))\n",
    "    .withColumn(\"hour\", F.hour(\"date_ts\"))\n",
    "    .withColumn(\"dayofweek\", F.date_format(\"date_ts\", \"E\"))\n",
    "    .withColumn(\n",
    "        \"season\",\n",
    "        F.when(F.col(\"month\").isin(12, 1, 2), \"winter\")\n",
    "         .when(F.col(\"month\").isin(3, 4, 5), \"spring\")\n",
    "         .when(F.col(\"month\").isin(6, 7, 8), \"summer\")\n",
    "         .otherwise(\"fall\")\n",
    "    )\n",
    "    .withColumn(\"lat_round\", F.round(\"latitude\", 3))\n",
    "    .withColumn(\"lon_round\", F.round(\"longitude\", 3))\n",
    ")\n",
    "\n",
    "print(\"✓ Feature engineering complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d687fe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for crime data file...\n",
      "✓ Found crime data: ../data/raw/chicago_crimes_2001_present.csv\n",
      "\n",
      "File size: 1.86 GB\n"
     ]
    }
   ],
   "source": [
    "# Find the crime data file\n",
    "import os\n",
    "\n",
    "print(\"Looking for crime data file...\")\n",
    "\n",
    "# Check different possible locations\n",
    "possible_paths = [\n",
    "    \"../data/raw/\",\n",
    "    \"./data/raw/\",\n",
    "    \"../data/\",\n",
    "    \"./\",\n",
    "]\n",
    "\n",
    "crime_file = None\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        files = [f for f in os.listdir(path) if 'crime' in f.lower() or 'Crimes' in f]\n",
    "        if files:\n",
    "            crime_file = os.path.join(path, files[0])\n",
    "            print(f\"✓ Found crime data: {crime_file}\")\n",
    "            break\n",
    "\n",
    "if not crime_file:\n",
    "    print(\"✗ Crime data not found. Please provide the path to your crime CSV file.\")\n",
    "else:\n",
    "    print(f\"\\nFile size: {os.path.getsize(crime_file) / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d0ca65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING CRIME DATA\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 2,000,000 crime records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 98:==================================>                      (9 + 6) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Clean records: 1,971,196\n",
      "✓ Feature engineering complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"LOADING CRIME DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Replace with your actual path from Step 1\n",
    "crime_path = crime_file  # Use the path found above\n",
    "\n",
    "# Load with limit for memory efficiency\n",
    "df = spark.read.csv(\n",
    "    crime_path,\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ").limit(2000000)  # 2M records\n",
    "\n",
    "print(f\"✓ Loaded {df.count():,} crime records\")\n",
    "\n",
    "# Convert date and add features\n",
    "df = df.withColumn(\"date_ts\", F.to_timestamp(\"Date\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "\n",
    "df = (\n",
    "    df\n",
    "    .withColumn(\"year\", F.year(\"date_ts\"))\n",
    "    .withColumn(\"month\", F.month(\"date_ts\"))\n",
    "    .withColumn(\"day\", F.dayofmonth(\"date_ts\"))\n",
    "    .withColumn(\"hour\", F.hour(\"date_ts\"))\n",
    "    .withColumn(\"dayofweek\", F.date_format(\"date_ts\", \"E\"))\n",
    "    .withColumn(\n",
    "        \"season\",\n",
    "        F.when(F.col(\"month\").isin(12, 1, 2), \"winter\")\n",
    "         .when(F.col(\"month\").isin(3, 4, 5), \"spring\")\n",
    "         .when(F.col(\"month\").isin(6, 7, 8), \"summer\")\n",
    "         .otherwise(\"fall\")\n",
    "    )\n",
    "    .withColumn(\"crime_date\", F.to_date(\"date_ts\"))  # Important for joining with weather\n",
    ")\n",
    "\n",
    "# Clean data\n",
    "df_clean = df.filter(\n",
    "    (F.col(\"latitude\").isNotNull()) & \n",
    "    (F.col(\"longitude\").isNotNull()) &\n",
    "    (F.col(\"date_ts\").isNotNull())\n",
    ")\n",
    "\n",
    "print(f\"✓ Clean records: {df_clean.count():,}\")\n",
    "print(\"✓ Feature engineering complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdcb7fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "INTEGRATING WEATHER DATA\n",
      "======================================================================\n",
      "✓ Loaded 8,766 days of weather data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Integrated crime + weather: 1,971,196 records\n",
      "\n",
      "Sample of integrated data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-11-10 14:35:54.703\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `primary_type` cannot be resolved. Did you mean one of the following? [`Primary Type`, `District`, `crime_date`, `date_ts`, `Arrest`]. SQLSTATE: 42703\", \"context\": {\"file\": \"jdk.internal.reflect.GeneratedMethodAccessor59.invoke(Unknown Source)\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o657.select.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `primary_type` cannot be resolved. Did you mean one of the following? [`Primary Type`, `District`, `crime_date`, `date_ts`, `Arrest`]. SQLSTATE: 42703;\\n'Project [date_ts#1553, 'primary_type, temp_mean#1617, precipitation#1618, wind_speed#1619]\\n+- Join LeftOuter, (crime_date#1560 = weather_date#1621)\\n   :- Filter ((isnotnull(latitude#1523) AND isnotnull(longitude#1524)) AND isnotnull(date_ts#1553))\\n   :  +- Project [ID#1504, Case Number#1505, Date#1506, Block#1507, IUCR#1508, Primary Type#1509, Description#1510, Location Description#1511, Arrest#1512, Domestic#1513, Beat#1514, District#1515, Ward#1516, Community Area#1517, FBI Code#1518, X Coordinate#1519, Y Coordinate#1520, year#1554, Updated On#1522, Latitude#1523, Longitude#1524, Location#1525, date_ts#1553, month#1555, day#1556, ... 4 more fields]\\n   :     +- Project [ID#1504, Case Number#1505, Date#1506, Block#1507, IUCR#1508, Primary Type#1509, Description#1510, Location Description#1511, Arrest#1512, Domestic#1513, Beat#1514, District#1515, Ward#1516, Community Area#1517, FBI Code#1518, X Coordinate#1519, Y Coordinate#1520, year#1554, Updated On#1522, Latitude#1523, Longitude#1524, Location#1525, date_ts#1553, month#1555, day#1556, ... 3 more fields]\\n   :        +- Project [ID#1504, Case Number#1505, Date#1506, Block#1507, IUCR#1508, Primary Type#1509, Description#1510, Location Description#1511, Arrest#1512, Domestic#1513, Beat#1514, District#1515, Ward#1516, Community Area#1517, FBI Code#1518, X Coordinate#1519, Y Coordinate#1520, year#1554, Updated On#1522, Latitude#1523, Longitude#1524, Location#1525, date_ts#1553, month#1555, day#1556, ... 2 more fields]\\n   :           +- Project [ID#1504, Case Number#1505, Date#1506, Block#1507, IUCR#1508, Primary Type#1509, Description#1510, Location Description#1511, Arrest#1512, Domestic#1513, Beat#1514, District#1515, Ward#1516, Community Area#1517, FBI Code#1518, X Coordinate#1519, Y Coordinate#1520, year#1554, Updated On#1522, Latitude#1523, Longitude#1524, Location#1525, date_ts#1553, month#1555, day#1556, ... 1 more fields]\\n   :              +- Project [ID#1504, Case Number#1505, Date#1506, Block#1507, IUCR#1508, Primary Type#1509, Description#1510, Location Description#1511, Arrest#1512, Domestic#1513, Beat#1514, District#1515, Ward#1516, Community Area#1517, FBI Code#1518, X Coordinate#1519, Y Coordinate#1520, year#1554, Updated On#1522, Latitude#1523, Longitude#1524, Location#1525, date_ts#1553, month#1555, dayofmonth(cast(date_ts#1553 as date)) AS day#1556]\\n   :                 +- Project [ID#1504, Case Number#1505, Date#1506, Block#1507, IUCR#1508, Primary Type#1509, Description#1510, Location Description#1511, Arrest#1512, Domestic#1513, Beat#1514, District#1515, Ward#1516, Community Area#1517, FBI Code#1518, X Coordinate#1519, Y Coordinate#1520, year#1554, Updated On#1522, Latitude#1523, Longitude#1524, Location#1525, date_ts#1553, month(cast(date_ts#1553 as date)) AS month#1555]\\n   :                    +- Project [ID#1504, Case Number#1505, Date#1506, Block#1507, IUCR#1508, Primary Type#1509, Description#1510, Location Description#1511, Arrest#1512, Domestic#1513, Beat#1514, District#1515, Ward#1516, Community Area#1517, FBI Code#1518, X Coordinate#1519, Y Coordinate#1520, year(cast(date_ts#1553 as date)) AS year#1554, Updated On#1522, Latitude#1523, Longitude#1524, Location#1525, date_ts#1553]\\n   :                       +- Project [ID#1504, Case Number#1505, Date#1506, Block#1507, IUCR#1508, Primary Type#1509, Description#1510, Location Description#1511, Arrest#1512, Domestic#1513, Beat#1514, District#1515, Ward#1516, Community Area#1517, FBI Code#1518, X Coordinate#1519, Y Coordinate#1520, Year#1521, Updated On#1522, Latitude#1523, Longitude#1524, Location#1525, to_timestamp(Date#1506, Some(MM/dd/yyyy hh:mm:ss a), TimestampType, Some(America/New_York), true) AS date_ts#1553]\\n   :                          +- GlobalLimit 2000000\\n   :                             +- LocalLimit 2000000\\n   :                                +- Relation [ID#1504,Case Number#1505,Date#1506,Block#1507,IUCR#1508,Primary Type#1509,Description#1510,Location Description#1511,Arrest#1512,Domestic#1513,Beat#1514,District#1515,Ward#1516,Community Area#1517,FBI Code#1518,X Coordinate#1519,Y Coordinate#1520,Year#1521,Updated On#1522,Latitude#1523,Longitude#1524,Location#1525] csv\\n   +- Project [date#1614, temp_max#1615, temp_min#1616, temp_mean#1617, precipitation#1618, wind_speed#1619, to_date(date#1614, None, Some(America/New_York), true) AS weather_date#1621]\\n      +- Relation [date#1614,temp_max#1615,temp_min#1616,temp_mean#1617,precipitation#1618,wind_speed#1619] csv\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2263)\\n\\tat org.apache.spark.sql.classic.Dataset.select(Dataset.scala:894)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 20 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `primary_type` cannot be resolved. Did you mean one of the following? [`Primary Type`, `District`, `crime_date`, `date_ts`, `Arrest`]. SQLSTATE: 42703;\n'Project [date_ts#1553, 'primary_type, temp_mean#1617, precipitation#1618, wind_speed#1619]\n+- Join LeftOuter, (crime_date#1560 = weather_date#1621)\n   :- Filter ((isnotnull(latitude#1523) AND isnotnull(longitude#1524)) AND isnotnull(date_ts#1553))\n   :  +- Project [ID#1504, Case Number#1505, Date#1506, Block#1507, IUCR#1508, Primary Type#1509, Description#1510, Location Description#1511, Arrest#1512, Domestic#1513, Beat#1514, District#1515, Ward#1516, Community Area#1517, FBI Code#1518, X Coordinate#1519, Y Coordinate#1520, year#1554, Updated On#1522, Latitude#1523, Longitude#1524, Location#1525, date_ts#1553, month#1555, day#1556, ... 4 more fields]\n   :     +- Project [ID#1504, Case Number#1505, Date#1506, Block#1507, IUCR#1508, Primary Type#1509, Description#1510, Location Description#1511, Arrest#1512, Domestic#1513, Beat#1514, District#1515, Ward#1516, Community Area#1517, FBI Code#1518, X Coordinate#1519, Y Coordinate#1520, year#1554, Updated On#1522, Latitude#1523, Longitude#1524, Location#1525, date_ts#1553, month#1555, day#1556, ... 3 more fields]\n   :        +- Project [ID#1504, Case Number#1505, Date#1506, Block#1507, IUCR#1508, Primary Type#1509, Description#1510, Location Description#1511, Arrest#1512, Domestic#1513, Beat#1514, District#1515, Ward#1516, Community Area#1517, FBI Code#1518, X Coordinate#1519, Y Coordinate#1520, year#1554, Updated On#1522, Latitude#1523, Longitude#1524, Location#1525, date_ts#1553, month#1555, day#1556, ... 2 more fields]\n   :           +- Project [ID#1504, Case Number#1505, Date#1506, Block#1507, IUCR#1508, Primary Type#1509, Description#1510, Location Description#1511, Arrest#1512, Domestic#1513, Beat#1514, District#1515, Ward#1516, Community Area#1517, FBI Code#1518, X Coordinate#1519, Y Coordinate#1520, year#1554, Updated On#1522, Latitude#1523, Longitude#1524, Location#1525, date_ts#1553, month#1555, day#1556, ... 1 more fields]\n   :              +- Project [ID#1504, Case Number#1505, Date#1506, Block#1507, IUCR#1508, Primary Type#1509, Description#1510, Location Description#1511, Arrest#1512, Domestic#1513, Beat#1514, District#1515, Ward#1516, Community Area#1517, FBI Code#1518, X Coordinate#1519, Y Coordinate#1520, year#1554, Updated On#1522, Latitude#1523, Longitude#1524, Location#1525, date_ts#1553, month#1555, dayofmonth(cast(date_ts#1553 as date)) AS day#1556]\n   :                 +- Project [ID#1504, Case Number#1505, Date#1506, Block#1507, IUCR#1508, Primary Type#1509, Description#1510, Location Description#1511, Arrest#1512, Domestic#1513, Beat#1514, District#1515, Ward#1516, Community Area#1517, FBI Code#1518, X Coordinate#1519, Y Coordinate#1520, year#1554, Updated On#1522, Latitude#1523, Longitude#1524, Location#1525, date_ts#1553, month(cast(date_ts#1553 as date)) AS month#1555]\n   :                    +- Project [ID#1504, Case Number#1505, Date#1506, Block#1507, IUCR#1508, Primary Type#1509, Description#1510, Location Description#1511, Arrest#1512, Domestic#1513, Beat#1514, District#1515, Ward#1516, Community Area#1517, FBI Code#1518, X Coordinate#1519, Y Coordinate#1520, year(cast(date_ts#1553 as date)) AS year#1554, Updated On#1522, Latitude#1523, Longitude#1524, Location#1525, date_ts#1553]\n   :                       +- Project [ID#1504, Case Number#1505, Date#1506, Block#1507, IUCR#1508, Primary Type#1509, Description#1510, Location Description#1511, Arrest#1512, Domestic#1513, Beat#1514, District#1515, Ward#1516, Community Area#1517, FBI Code#1518, X Coordinate#1519, Y Coordinate#1520, Year#1521, Updated On#1522, Latitude#1523, Longitude#1524, Location#1525, to_timestamp(Date#1506, Some(MM/dd/yyyy hh:mm:ss a), TimestampType, Some(America/New_York), true) AS date_ts#1553]\n   :                          +- GlobalLimit 2000000\n   :                             +- LocalLimit 2000000\n   :                                +- Relation [ID#1504,Case Number#1505,Date#1506,Block#1507,IUCR#1508,Primary Type#1509,Description#1510,Location Description#1511,Arrest#1512,Domestic#1513,Beat#1514,District#1515,Ward#1516,Community Area#1517,FBI Code#1518,X Coordinate#1519,Y Coordinate#1520,Year#1521,Updated On#1522,Latitude#1523,Longitude#1524,Location#1525] csv\n   +- Project [date#1614, temp_max#1615, temp_min#1616, temp_mean#1617, precipitation#1618, wind_speed#1619, to_date(date#1614, None, Some(America/New_York), true) AS weather_date#1621]\n      +- Relation [date#1614,temp_max#1615,temp_min#1616,temp_mean#1617,precipitation#1618,wind_speed#1619] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Show sample\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSample of integrated data:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mdf_integrated\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdate_ts\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprimary_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemp_mean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprecipitation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwind_speed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m.show(\u001b[32m5\u001b[39m, truncate=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages/pyspark/sql/classic/dataframe.py:991\u001b[39m, in \u001b[36mDataFrame.select\u001b[39m\u001b[34m(self, *cols)\u001b[39m\n\u001b[32m    990\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cols: \u001b[33m\"\u001b[39m\u001b[33mColumnOrName\u001b[39m\u001b[33m\"\u001b[39m) -> ParentDataFrame:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     jdf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    992\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `primary_type` cannot be resolved. Did you mean one of the following? [`Primary Type`, `District`, `crime_date`, `date_ts`, `Arrest`]. SQLSTATE: 42703;\n'Project [date_ts#1553, 'primary_type, temp_mean#1617, precipitation#1618, wind_speed#1619]\n+- Join LeftOuter, (crime_date#1560 = weather_date#1621)\n   :- Filter ((isnotnull(latitude#1523) AND isnotnull(longitude#1524)) AND isnotnull(date_ts#1553))\n   :  +- Project [ID#1504, Case Number#1505, Date#1506, Block#1507, IUCR#1508, Primary Type#1509, Description#1510, Location Description#1511, Arrest#1512, Domestic#1513, Beat#1514, District#1515, Ward#1516, Community Area#1517, FBI Code#1518, X Coordinate#1519, Y Coordinate#1520, year#1554, Updated On#1522, Latitude#1523, Longitude#1524, Location#1525, date_ts#1553, month#1555, day#1556, ... 4 more fields]\n   :     +- Project [ID#1504, Case Number#1505, Date#1506, Block#1507, IUCR#1508, Primary Type#1509, Description#1510, Location Description#1511, Arrest#1512, Domestic#1513, Beat#1514, District#1515, Ward#1516, Community Area#1517, FBI Code#1518, X Coordinate#1519, Y Coordinate#1520, year#1554, Updated On#1522, Latitude#1523, Longitude#1524, Location#1525, date_ts#1553, month#1555, day#1556, ... 3 more fields]\n   :        +- Project [ID#1504, Case Number#1505, Date#1506, Block#1507, IUCR#1508, Primary Type#1509, Description#1510, Location Description#1511, Arrest#1512, Domestic#1513, Beat#1514, District#1515, Ward#1516, Community Area#1517, FBI Code#1518, X Coordinate#1519, Y Coordinate#1520, year#1554, Updated On#1522, Latitude#1523, Longitude#1524, Location#1525, date_ts#1553, month#1555, day#1556, ... 2 more fields]\n   :           +- Project [ID#1504, Case Number#1505, Date#1506, Block#1507, IUCR#1508, Primary Type#1509, Description#1510, Location Description#1511, Arrest#1512, Domestic#1513, Beat#1514, District#1515, Ward#1516, Community Area#1517, FBI Code#1518, X Coordinate#1519, Y Coordinate#1520, year#1554, Updated On#1522, Latitude#1523, Longitude#1524, Location#1525, date_ts#1553, month#1555, day#1556, ... 1 more fields]\n   :              +- Project [ID#1504, Case Number#1505, Date#1506, Block#1507, IUCR#1508, Primary Type#1509, Description#1510, Location Description#1511, Arrest#1512, Domestic#1513, Beat#1514, District#1515, Ward#1516, Community Area#1517, FBI Code#1518, X Coordinate#1519, Y Coordinate#1520, year#1554, Updated On#1522, Latitude#1523, Longitude#1524, Location#1525, date_ts#1553, month#1555, dayofmonth(cast(date_ts#1553 as date)) AS day#1556]\n   :                 +- Project [ID#1504, Case Number#1505, Date#1506, Block#1507, IUCR#1508, Primary Type#1509, Description#1510, Location Description#1511, Arrest#1512, Domestic#1513, Beat#1514, District#1515, Ward#1516, Community Area#1517, FBI Code#1518, X Coordinate#1519, Y Coordinate#1520, year#1554, Updated On#1522, Latitude#1523, Longitude#1524, Location#1525, date_ts#1553, month(cast(date_ts#1553 as date)) AS month#1555]\n   :                    +- Project [ID#1504, Case Number#1505, Date#1506, Block#1507, IUCR#1508, Primary Type#1509, Description#1510, Location Description#1511, Arrest#1512, Domestic#1513, Beat#1514, District#1515, Ward#1516, Community Area#1517, FBI Code#1518, X Coordinate#1519, Y Coordinate#1520, year(cast(date_ts#1553 as date)) AS year#1554, Updated On#1522, Latitude#1523, Longitude#1524, Location#1525, date_ts#1553]\n   :                       +- Project [ID#1504, Case Number#1505, Date#1506, Block#1507, IUCR#1508, Primary Type#1509, Description#1510, Location Description#1511, Arrest#1512, Domestic#1513, Beat#1514, District#1515, Ward#1516, Community Area#1517, FBI Code#1518, X Coordinate#1519, Y Coordinate#1520, Year#1521, Updated On#1522, Latitude#1523, Longitude#1524, Location#1525, to_timestamp(Date#1506, Some(MM/dd/yyyy hh:mm:ss a), TimestampType, Some(America/New_York), true) AS date_ts#1553]\n   :                          +- GlobalLimit 2000000\n   :                             +- LocalLimit 2000000\n   :                                +- Relation [ID#1504,Case Number#1505,Date#1506,Block#1507,IUCR#1508,Primary Type#1509,Description#1510,Location Description#1511,Arrest#1512,Domestic#1513,Beat#1514,District#1515,Ward#1516,Community Area#1517,FBI Code#1518,X Coordinate#1519,Y Coordinate#1520,Year#1521,Updated On#1522,Latitude#1523,Longitude#1524,Location#1525] csv\n   +- Project [date#1614, temp_max#1615, temp_min#1616, temp_mean#1617, precipitation#1618, wind_speed#1619, to_date(date#1614, None, Some(America/New_York), true) AS weather_date#1621]\n      +- Relation [date#1614,temp_max#1615,temp_min#1616,temp_mean#1617,precipitation#1618,wind_speed#1619] csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTEGRATING WEATHER DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load weather data\n",
    "weather_path = \"../data/raw/chicago_weather_2001_2024.csv\"\n",
    "weather_spark = spark.read.csv(weather_path, header=True, inferSchema=True)\n",
    "\n",
    "# Convert weather date to date type\n",
    "weather_spark = weather_spark.withColumn(\"weather_date\", F.to_date(\"date\"))\n",
    "\n",
    "print(f\"✓ Loaded {weather_spark.count():,} days of weather data\")\n",
    "\n",
    "# Join crime data with weather data\n",
    "df_integrated = df_clean.join(\n",
    "    weather_spark,\n",
    "    df_clean.crime_date == weather_spark.weather_date,\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "print(f\"✓ Integrated crime + weather: {df_integrated.count():,} records\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample of integrated data:\")\n",
    "df_integrated.select(\n",
    "    \"date_ts\", \"primary_type\", \"temp_mean\", \"precipitation\", \"wind_speed\"\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7319ff79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of integrated data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 110:=================================>                      (9 + 6) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+---------+-------------+----------+\n",
      "|date_ts            |Primary Type   |temp_mean|precipitation|wind_speed|\n",
      "+-------------------+---------------+---------+-------------+----------+\n",
      "|2020-08-10 09:45:00|ROBBERY        |75.2     |0.295        |14.6      |\n",
      "|2023-09-06 17:00:00|CRIMINAL DAMAGE|75.7     |0.331        |19.8      |\n",
      "|2023-09-06 11:00:00|THEFT          |75.7     |0.331        |19.8      |\n",
      "|2019-05-21 08:20:00|BURGLARY       |49.1     |0.409        |21.9      |\n",
      "|2021-07-07 10:30:00|SEX OFFENSE    |75.2     |0.15         |12.4      |\n",
      "+-------------------+---------------+---------+-------------+----------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show sample with correct column names (use backticks for columns with spaces)\n",
    "print(\"\\nSample of integrated data:\")\n",
    "df_integrated.select(\n",
    "    \"date_ts\", \n",
    "    F.col(\"Primary Type\"),  # Correct column name\n",
    "    \"temp_mean\", \n",
    "    \"precipitation\", \n",
    "    \"wind_speed\"\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e457982f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "INTEGRATING CENSUS/DEMOGRAPHIC DATA\n",
      "======================================================================\n",
      "✓ Loaded census data: 78 community areas\n",
      "\n",
      "Census columns available:\n",
      "root\n",
      " |-- ca: integer (nullable = true)\n",
      " |-- community_area_name: string (nullable = true)\n",
      " |-- percent_of_housing_crowded: double (nullable = true)\n",
      " |-- percent_households_below_poverty: double (nullable = true)\n",
      " |-- percent_aged_16_unemployed: double (nullable = true)\n",
      " |-- percent_aged_25_without_high_school_diploma: double (nullable = true)\n",
      " |-- percent_aged_under_18_or_over_64: double (nullable = true)\n",
      " |-- per_capita_income_: integer (nullable = true)\n",
      " |-- hardship_index: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 133:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Final integrated dataset: 1,971,196 records\n",
      "✓ Final dataset cached\n",
      "\n",
      "======================================================================\n",
      "✓ ALL DATA INTEGRATION COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Sample of fully integrated data:\n",
      "+-------------------+-------------------+---------+-------------+--------------+------------------+\n",
      "|date_ts            |Primary Type       |temp_mean|precipitation|Community Area|per_capita_income_|\n",
      "+-------------------+-------------------+---------+-------------+--------------+------------------+\n",
      "|2020-08-10 09:45:00|ROBBERY            |75.2     |0.295        |24            |43198             |\n",
      "|2023-09-06 17:00:00|CRIMINAL DAMAGE    |75.7     |0.331        |32            |65526             |\n",
      "|2023-09-06 11:00:00|THEFT              |75.7     |0.331        |32            |65526             |\n",
      "|2019-05-21 08:20:00|BURGLARY           |49.1     |0.409        |29            |12034             |\n",
      "|2021-07-07 10:30:00|SEX OFFENSE        |75.2     |0.15         |54            |8201              |\n",
      "|2022-06-14 14:47:00|ROBBERY            |84.8     |0.0          |15            |24336             |\n",
      "|2022-09-21 22:00:00|MOTOR VEHICLE THEFT|74.0     |0.0          |69            |17285             |\n",
      "|2023-02-22 13:50:00|BURGLARY           |32.8     |1.22         |43            |19398             |\n",
      "|2023-05-03 08:10:00|BATTERY            |42.5     |0.0          |68            |11888             |\n",
      "|2022-12-25 00:01:00|BATTERY            |8.5      |0.0          |63            |12171             |\n",
      "+-------------------+-------------------+---------+-------------+--------------+------------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/10 14:40:54 WARN CacheManager: Asked to cache already cached data.        \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTEGRATING CENSUS/DEMOGRAPHIC DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load census data - USE THE CORRECT FILENAME\n",
    "census_path = \"../data/raw/chicago_census_community_areas.csv\"  # ← FIXED PATH\n",
    "census_spark = spark.read.csv(census_path, header=True, inferSchema=True)\n",
    "\n",
    "print(f\"✓ Loaded census data: {census_spark.count()} community areas\")\n",
    "\n",
    "# Show census columns to see what's available\n",
    "print(\"\\nCensus columns available:\")\n",
    "census_spark.printSchema()\n",
    "\n",
    "# The census data has \"ca\" column for community area number\n",
    "# Join with crime data by Community Area\n",
    "df_final = df_integrated.join(\n",
    "    census_spark,\n",
    "    df_integrated[\"Community Area\"] == census_spark[\"ca\"],  # ← Match column names\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "print(f\"✓ Final integrated dataset: {df_final.count():,} records\")\n",
    "\n",
    "# Cache the final dataset\n",
    "df_final.cache()\n",
    "print(\"✓ Final dataset cached\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ ALL DATA INTEGRATION COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show sample with all data sources - CORRECT COLUMN NAMES\n",
    "print(\"\\nSample of fully integrated data:\")\n",
    "df_final.select(\n",
    "    \"date_ts\", \n",
    "    F.col(\"Primary Type\"),\n",
    "    \"temp_mean\", \n",
    "    \"precipitation\", \n",
    "    F.col(\"Community Area\"),\n",
    "    F.col(\"per_capita_income_\")  # ← Fixed: added underscore\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae0fc4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "WEEK 2 - FINAL STATISTICAL SUMMARY REPORT\n",
      "Team: Kiran Ghumare, Neethu Satravada, Sajitha Mathi\n",
      "======================================================================\n",
      "\n",
      "✅ DATASET OVERVIEW:\n",
      "   Total integrated records: 1,971,196\n",
      "   Weather data: 8,766 days (2001-2024)\n",
      "   Census areas: 78 community areas\n",
      "   Date range: 2001-2024\n",
      "\n",
      "✅ DATA SOURCES INTEGRATED:\n",
      "   ✓ Crime data (Chicago Police Department)\n",
      "   ✓ Weather data (Open-Meteo API)\n",
      "   ✓ Census/demographic data (Chicago Data Portal)\n",
      "\n",
      "✅ FEATURES ENGINEERED:\n",
      "   Temporal: year, month, day, hour, day_of_week, season\n",
      "   Weather: temperature, precipitation, wind_speed\n",
      "   Demographic: per_capita_income, hardship_index, poverty rates\n",
      "   Spatial: latitude, longitude, community_area\n",
      "\n",
      "======================================================================\n",
      "KEY INSIGHTS & PATTERNS\n",
      "======================================================================\n",
      "\n",
      "1. TOP 10 CRIME TYPES:\n",
      "+-------------------+------+\n",
      "|Primary Type       |count |\n",
      "+-------------------+------+\n",
      "|THEFT              |415186|\n",
      "|BATTERY            |385291|\n",
      "|CRIMINAL DAMAGE    |233811|\n",
      "|NARCOTICS          |201587|\n",
      "|ASSAULT            |131088|\n",
      "|MOTOR VEHICLE THEFT|107739|\n",
      "|BURGLARY           |106393|\n",
      "|OTHER OFFENSE      |89418 |\n",
      "|ROBBERY            |75596 |\n",
      "|DECEPTIVE PRACTICE |61910 |\n",
      "+-------------------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "2. CRIMES BY SEASON (with average weather):\n",
      "+------+-----------+----------+---------------+\n",
      "|season|crime_count|avg_temp_F|avg_precip_inch|\n",
      "+------+-----------+----------+---------------+\n",
      "|  fall|     533792|      54.7|           0.09|\n",
      "|summer|     502622|      72.1|           0.11|\n",
      "|spring|     476879|      45.7|           0.11|\n",
      "|winter|     457903|      28.0|           0.05|\n",
      "+------+-----------+----------+---------------+\n",
      "\n",
      "\n",
      "3. PEAK CRIME HOURS:\n",
      "+----+------+\n",
      "|hour| count|\n",
      "+----+------+\n",
      "|  20|115060|\n",
      "|  21|113232|\n",
      "|  19|112611|\n",
      "|  22|111867|\n",
      "|   0|110574|\n",
      "+----+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "4. CRIMES BY DAY OF WEEK:\n",
      "+---------+------+\n",
      "|dayofweek| count|\n",
      "+---------+------+\n",
      "|      Fri|294812|\n",
      "|      Wed|283947|\n",
      "|      Sat|282477|\n",
      "|      Tue|282430|\n",
      "|      Thu|281651|\n",
      "|      Mon|279856|\n",
      "|      Sun|266023|\n",
      "+---------+------+\n",
      "\n",
      "\n",
      "5. WEATHER IMPACT ON CRIME:\n",
      "   High Temperature Days (>80°F) vs Low Temperature Days (<40°F):\n",
      "   High temp crimes: 50,821\n",
      "   Low temp crimes: 634,768\n",
      "   Ratio: 0.08x more crimes in hot weather\n",
      "\n",
      "6. CRIME BY INCOME LEVELS:\n",
      "+--------------------+------+\n",
      "|        income_level| count|\n",
      "+--------------------+------+\n",
      "| High Income (>$30k)|935487|\n",
      "|Medium Income ($1...|680754|\n",
      "|  Low Income (<$15k)|354955|\n",
      "+--------------------+------+\n",
      "\n",
      "\n",
      "======================================================================\n",
      "✅ WEEK 2 DELIVERABLES COMPLETED!\n",
      "======================================================================\n",
      "\n",
      "📋 DELIVERABLES CHECKLIST:\n",
      "   ✅ Clean, integrated dataset created\n",
      "      - Crime data cleaned and validated\n",
      "      - Weather data integrated by date\n",
      "      - Census data integrated by community area\n",
      "   ✅ Feature engineering completed\n",
      "      - Temporal features (year, month, hour, season, etc.)\n",
      "      - Weather features (temperature, precipitation, wind)\n",
      "      - Demographic features (income, poverty, education)\n",
      "   ✅ Exploratory Data Analysis completed\n",
      "      - Crime patterns by time analyzed\n",
      "      - Weather impact assessed\n",
      "      - Socioeconomic correlations identified\n",
      "   ✅ Statistical summary report generated\n",
      "\n",
      "🎯 READY FOR WEEK 3: Geospatial Analysis & Hotspot Detection\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WEEK 2 - FINAL STATISTICAL SUMMARY REPORT\")\n",
    "print(\"Team: Kiran Ghumare, Neethu Satravada, Sajitha Mathi\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n✅ DATASET OVERVIEW:\")\n",
    "print(f\"   Total integrated records: {df_final.count():,}\")\n",
    "print(f\"   Weather data: 8,766 days (2001-2024)\")\n",
    "print(f\"   Census areas: {census_spark.count()} community areas\")\n",
    "print(f\"   Date range: 2001-2024\")\n",
    "\n",
    "print(f\"\\n✅ DATA SOURCES INTEGRATED:\")\n",
    "print(f\"   ✓ Crime data (Chicago Police Department)\")\n",
    "print(f\"   ✓ Weather data (Open-Meteo API)\")\n",
    "print(f\"   ✓ Census/demographic data (Chicago Data Portal)\")\n",
    "\n",
    "print(f\"\\n✅ FEATURES ENGINEERED:\")\n",
    "print(f\"   Temporal: year, month, day, hour, day_of_week, season\")\n",
    "print(f\"   Weather: temperature, precipitation, wind_speed\")\n",
    "print(f\"   Demographic: per_capita_income, hardship_index, poverty rates\")\n",
    "print(f\"   Spatial: latitude, longitude, community_area\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHTS & PATTERNS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Top 10 Crime Types\n",
    "print(\"\\n1. TOP 10 CRIME TYPES:\")\n",
    "df_final.groupBy(F.col(\"Primary Type\")).count() \\\n",
    "    .orderBy(F.desc(\"count\")) \\\n",
    "    .show(10, truncate=False)\n",
    "\n",
    "# 2. Crimes by Season with Weather\n",
    "print(\"\\n2. CRIMES BY SEASON (with average weather):\")\n",
    "df_final.groupBy(\"season\").agg(\n",
    "    F.count(\"*\").alias(\"crime_count\"),\n",
    "    F.round(F.avg(\"temp_mean\"), 1).alias(\"avg_temp_F\"),\n",
    "    F.round(F.avg(\"precipitation\"), 2).alias(\"avg_precip_inch\")\n",
    ").orderBy(F.desc(\"crime_count\")).show()\n",
    "\n",
    "# 3. Peak Crime Hours\n",
    "print(\"\\n3. PEAK CRIME HOURS:\")\n",
    "df_final.groupBy(\"hour\").count() \\\n",
    "    .orderBy(F.desc(\"count\")) \\\n",
    "    .show(5)\n",
    "\n",
    "# 4. Crime by Day of Week\n",
    "print(\"\\n4. CRIMES BY DAY OF WEEK:\")\n",
    "df_final.groupBy(\"dayofweek\").count() \\\n",
    "    .orderBy(F.desc(\"count\")) \\\n",
    "    .show()\n",
    "\n",
    "# 5. Weather Impact Analysis\n",
    "print(\"\\n5. WEATHER IMPACT ON CRIME:\")\n",
    "print(\"   High Temperature Days (>80°F) vs Low Temperature Days (<40°F):\")\n",
    "high_temp = df_final.filter(F.col(\"temp_mean\") > 80).count()\n",
    "low_temp = df_final.filter(F.col(\"temp_mean\") < 40).count()\n",
    "print(f\"   High temp crimes: {high_temp:,}\")\n",
    "print(f\"   Low temp crimes: {low_temp:,}\")\n",
    "print(f\"   Ratio: {high_temp/low_temp:.2f}x more crimes in hot weather\")\n",
    "\n",
    "# 6. Socioeconomic Analysis\n",
    "print(\"\\n6. CRIME BY INCOME LEVELS:\")\n",
    "df_final.groupBy(\n",
    "    F.when(F.col(\"per_capita_income_\") < 15000, \"Low Income (<$15k)\")\n",
    "     .when((F.col(\"per_capita_income_\") >= 15000) & (F.col(\"per_capita_income_\") < 30000), \"Medium Income ($15k-$30k)\")\n",
    "     .otherwise(\"High Income (>$30k)\")\n",
    "     .alias(\"income_level\")\n",
    ").count().orderBy(F.desc(\"count\")).show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ WEEK 2 DELIVERABLES COMPLETED!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n📋 DELIVERABLES CHECKLIST:\")\n",
    "print(\"   ✅ Clean, integrated dataset created\")\n",
    "print(\"      - Crime data cleaned and validated\")\n",
    "print(\"      - Weather data integrated by date\")\n",
    "print(\"      - Census data integrated by community area\")\n",
    "print(\"   ✅ Feature engineering completed\")\n",
    "print(\"      - Temporal features (year, month, hour, season, etc.)\")\n",
    "print(\"      - Weather features (temperature, precipitation, wind)\")\n",
    "print(\"      - Demographic features (income, poverty, education)\")\n",
    "print(\"   ✅ Exploratory Data Analysis completed\")\n",
    "print(\"      - Crime patterns by time analyzed\")\n",
    "print(\"      - Weather impact assessed\")\n",
    "print(\"      - Socioeconomic correlations identified\")\n",
    "print(\"   ✅ Statistical summary report generated\")\n",
    "\n",
    "print(\"\\n🎯 READY FOR WEEK 3: Geospatial Analysis & Hotspot Detection\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54340389",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[COLUMN_ALREADY_EXISTS] The column `date` already exists. Choose another name or rename the existing column. SQLSTATE: 42711",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_final\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/processed/integrated_crime_data.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moverwrite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ Saved integrated dataset for Week 3!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages/pyspark/sql/readwriter.py:2003\u001b[39m, in \u001b[36mDataFrameWriter.parquet\u001b[39m\u001b[34m(self, path, mode, partitionBy, compression)\u001b[39m\n\u001b[32m   2001\u001b[39m     \u001b[38;5;28mself\u001b[39m.partitionBy(partitionBy)\n\u001b[32m   2002\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(compression=compression)\n\u001b[32m-> \u001b[39m\u001b[32m2003\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [COLUMN_ALREADY_EXISTS] The column `date` already exists. Choose another name or rename the existing column. SQLSTATE: 42711"
     ]
    }
   ],
   "source": [
    "df_final.write.parquet(\"../data/processed/integrated_crime_data.parquet\", mode=\"overwrite\")\n",
    "print(\"✓ Saved integrated dataset for Week 3!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56b74a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SAVING WEEK 2 DATA FOR WEEK 3\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 158:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved integrated dataset to: ../data/processed/integrated_crime_data.parquet\n",
      "✓ Records saved: 1,971,196\n",
      "\n",
      "✅ Week 2 data preserved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the integrated dataset from Week 2 - FIXED\n",
    "print(\"=\"*70)\n",
    "print(\"SAVING WEEK 2 DATA FOR WEEK 3\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select only the columns we need (avoid duplicates)\n",
    "df_to_save = df_final.select(\n",
    "    \"date_ts\",\n",
    "    F.col(\"Primary Type\").alias(\"crime_type\"),\n",
    "    F.col(\"Latitude\").alias(\"latitude\"),\n",
    "    F.col(\"Longitude\").alias(\"longitude\"),\n",
    "    F.col(\"Community Area\").alias(\"community_area\"),\n",
    "    \"temp_mean\",\n",
    "    \"precipitation\",\n",
    "    \"wind_speed\",\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"hour\",\n",
    "    \"dayofweek\",\n",
    "    \"season\",\n",
    "    F.col(\"per_capita_income_\").alias(\"per_capita_income\")\n",
    ")\n",
    "\n",
    "# Save to Parquet\n",
    "output_path = \"../data/processed/integrated_crime_data.parquet\"\n",
    "df_to_save.write.parquet(output_path, mode=\"overwrite\")\n",
    "\n",
    "print(f\"✓ Saved integrated dataset to: {output_path}\")\n",
    "print(f\"✓ Records saved: {df_to_save.count():,}\")\n",
    "print(\"\\n✅ Week 2 data preserved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f27700b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting h3\n",
      "  Downloading h3-4.3.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Collecting folium\n",
      "  Downloading folium-0.20.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp313-cp313-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: matplotlib in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (0.13.2)\n",
      "Collecting geopandas\n",
      "  Downloading geopandas-1.1.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting branca>=0.6.0 (from folium)\n",
      "  Downloading branca-0.8.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: jinja2>=2.9 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from folium) (3.1.6)\n",
      "Requirement already satisfied: numpy in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from folium) (2.3.4)\n",
      "Requirement already satisfied: requests in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from folium) (2.32.5)\n",
      "Collecting xyzservices (from folium)\n",
      "  Downloading xyzservices-2025.10.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.3-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from seaborn) (2.3.3)\n",
      "Collecting pyogrio>=0.7.2 (from geopandas)\n",
      "  Downloading pyogrio-0.11.1-cp313-cp313-macosx_12_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting pyproj>=3.5.0 (from geopandas)\n",
      "  Downloading pyproj-3.7.2-cp313-cp313-macosx_14_0_arm64.whl.metadata (31 kB)\n",
      "Collecting shapely>=2.0.0 (from geopandas)\n",
      "  Downloading shapely-2.1.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from jinja2>=2.9->folium) (3.0.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: certifi in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from pyogrio>=0.7.2->geopandas) (2025.10.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from requests->folium) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from requests->folium) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from requests->folium) (2.5.0)\n",
      "Downloading h3-4.3.1-cp313-cp313-macosx_11_0_arm64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.5/792.5 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading folium-0.20.0-py2.py3-none-any.whl (113 kB)\n",
      "Downloading scikit_learn-1.7.2-cp313-cp313-macosx_12_0_arm64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading geopandas-1.1.1-py3-none-any.whl (338 kB)\n",
      "Downloading branca-0.8.2-py3-none-any.whl (26 kB)\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading pyogrio-0.11.1-cp313-cp313-macosx_12_0_arm64.whl (19.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyproj-3.7.2-cp313-cp313-macosx_14_0_arm64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.16.3-cp313-cp313-macosx_14_0_arm64.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading shapely-2.1.2-cp313-cp313-macosx_11_0_arm64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading xyzservices-2025.10.0-py3-none-any.whl (92 kB)\n",
      "Installing collected packages: xyzservices, threadpoolctl, shapely, scipy, pyproj, pyogrio, joblib, h3, scikit-learn, branca, geopandas, folium\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/12\u001b[0m [folium]10/12\u001b[0m [geopandas]rn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed branca-0.8.2 folium-0.20.0 geopandas-1.1.1 h3-4.3.1 joblib-1.5.2 pyogrio-0.11.1 pyproj-3.7.2 scikit-learn-1.7.2 scipy-1.16.3 shapely-2.1.2 threadpoolctl-3.6.0 xyzservices-2025.10.0\n",
      "Collecting pysal\n",
      "  Downloading pysal-25.7-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting esda\n",
      "  Downloading esda-2.8.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting libpysal\n",
      "  Downloading libpysal-4.13.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.10 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from pysal) (4.14.2)\n",
      "Requirement already satisfied: geopandas>=0.10.0 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from pysal) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.22 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from pysal) (2.3.4)\n",
      "Requirement already satisfied: packaging>=22 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from pysal) (25.0)\n",
      "Requirement already satisfied: pandas>=1.4 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from pysal) (2.3.3)\n",
      "Requirement already satisfied: platformdirs>=2.0.2 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from pysal) (4.5.0)\n",
      "Requirement already satisfied: requests>=2.27 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from pysal) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.8 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from pysal) (1.16.3)\n",
      "Requirement already satisfied: shapely>=2.0.1 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from pysal) (2.1.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from pysal) (1.7.2)\n",
      "Collecting access>=1.1.9 (from pysal)\n",
      "  Downloading access-1.1.9-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting giddy>=2.3.6 (from pysal)\n",
      "  Downloading giddy-2.3.6-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting inequality>=1.1.2 (from pysal)\n",
      "  Downloading inequality-1.1.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting pointpats>=2.5.1 (from pysal)\n",
      "  Downloading pointpats-2.5.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting segregation>=2.5.2 (from pysal)\n",
      "  Downloading segregation-2.5.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting spaghetti>=1.7.6 (from pysal)\n",
      "  Downloading spaghetti-1.7.6-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mgwr>=2.2.1 (from pysal)\n",
      "  Downloading mgwr-2.2.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting momepy>=0.10.0 (from pysal)\n",
      "  Downloading momepy-0.10.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting spglm>=1.1.0 (from pysal)\n",
      "  Downloading spglm-1.1.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting spint>=1.0.7 (from pysal)\n",
      "  Downloading spint-1.0.7.tar.gz (28 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting spreg>=1.8.3 (from pysal)\n",
      "  Downloading spreg-1.8.3-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tobler>=0.12.1 (from pysal)\n",
      "  Downloading tobler-0.12.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting mapclassify>=2.10.0 (from pysal)\n",
      "  Downloading mapclassify-2.10.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting splot>=1.1.7 (from pysal)\n",
      "  Downloading splot-1.1.7-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting spopt>=0.7.0 (from pysal)\n",
      "  Downloading spopt-0.7.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from beautifulsoup4>=4.10->pysal) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from beautifulsoup4>=4.10->pysal) (4.15.0)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from geopandas>=0.10.0->pysal) (0.11.1)\n",
      "Requirement already satisfied: pyproj>=3.5.0 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from geopandas>=0.10.0->pysal) (3.7.2)\n",
      "Collecting quantecon>=0.7 (from giddy>=2.3.6->pysal)\n",
      "  Downloading quantecon-0.10.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: matplotlib>=3.8 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from inequality>=1.1.2->pysal) (3.10.7)\n",
      "Collecting networkx>=3.2 (from mapclassify>=2.10.0->pysal)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (2.9.0.post0)\n",
      "Collecting tqdm>=4.65 (from momepy>=0.10.0->pysal)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from pandas>=1.4->pysal) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from pandas>=1.4->pysal) (2025.2)\n",
      "Requirement already satisfied: certifi in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from pyogrio>=0.7.2->geopandas>=0.10.0->pysal) (2025.10.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib>=3.8->inequality>=1.1.2->pysal) (1.17.0)\n",
      "Collecting numba>=0.49.0 (from quantecon>=0.7->giddy>=2.3.6->pysal)\n",
      "  Downloading numba-0.62.1-cp313-cp313-macosx_12_0_arm64.whl.metadata (2.8 kB)\n",
      "Collecting sympy (from quantecon>=0.7->giddy>=2.3.6->pysal)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting llvmlite<0.46,>=0.45.0dev0 (from numba>=0.49.0->quantecon>=0.7->giddy>=2.3.6->pysal)\n",
      "  Downloading llvmlite-0.45.1-cp313-cp313-macosx_12_0_arm64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from requests>=2.27->pysal) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from requests>=2.27->pysal) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from requests>=2.27->pysal) (2.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from scikit-learn>=1.1->pysal) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from scikit-learn>=1.1->pysal) (3.6.0)\n",
      "Collecting deprecation (from segregation>=2.5.2->pysal)\n",
      "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: seaborn in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from segregation>=2.5.2->pysal) (0.13.2)\n",
      "Collecting rtree>=1.0 (from spaghetti>=1.7.6->pysal)\n",
      "  Downloading rtree-1.4.1-py3-none-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting pulp>=2.8 (from spopt>=0.7.0->pysal)\n",
      "  Downloading pulp-3.3.0-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting rasterio (from tobler>=0.12.1->pysal)\n",
      "  Downloading rasterio-1.4.3-cp313-cp313-macosx_14_0_arm64.whl.metadata (9.1 kB)\n",
      "Collecting statsmodels (from tobler>=0.12.1->pysal)\n",
      "  Downloading statsmodels-0.14.5-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.5 kB)\n",
      "Collecting rasterstats (from tobler>=0.12.1->pysal)\n",
      "  Downloading rasterstats-0.20.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting affine (from rasterio->tobler>=0.12.1->pysal)\n",
      "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (from rasterio->tobler>=0.12.1->pysal) (25.4.0)\n",
      "Collecting click>=4.0 (from rasterio->tobler>=0.12.1->pysal)\n",
      "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting cligj>=0.5 (from rasterio->tobler>=0.12.1->pysal)\n",
      "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting click-plugins (from rasterio->tobler>=0.12.1->pysal)\n",
      "  Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting fiona (from rasterstats->tobler>=0.12.1->pysal)\n",
      "  Downloading fiona-1.10.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (56 kB)\n",
      "Collecting simplejson (from rasterstats->tobler>=0.12.1->pysal)\n",
      "  Downloading simplejson-3.20.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (3.4 kB)\n",
      "Collecting patsy>=0.5.6 (from statsmodels->tobler>=0.12.1->pysal)\n",
      "  Downloading patsy-1.0.2-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->quantecon>=0.7->giddy>=2.3.6->pysal)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading pysal-25.7-py3-none-any.whl (17 kB)\n",
      "Downloading esda-2.8.0-py3-none-any.whl (157 kB)\n",
      "Downloading libpysal-4.13.0-py3-none-any.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading access-1.1.9-py3-none-any.whl (21 kB)\n",
      "Downloading giddy-2.3.6-py3-none-any.whl (61 kB)\n",
      "Downloading inequality-1.1.2-py3-none-any.whl (21 kB)\n",
      "Downloading mapclassify-2.10.0-py3-none-any.whl (882 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m882.2/882.2 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mgwr-2.2.1-py3-none-any.whl (47 kB)\n",
      "Downloading momepy-0.10.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pointpats-2.5.2-py3-none-any.whl (63 kB)\n",
      "Downloading quantecon-0.10.1-py3-none-any.whl (325 kB)\n",
      "Downloading numba-0.62.1-cp313-cp313-macosx_12_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.45.1-cp313-cp313-macosx_12_0_arm64.whl (37.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading segregation-2.5.2-py3-none-any.whl (141 kB)\n",
      "Downloading spaghetti-1.7.6-py3-none-any.whl (53 kB)\n",
      "Downloading rtree-1.4.1-py3-none-macosx_11_0_arm64.whl (436 kB)\n",
      "Downloading spglm-1.1.0-py3-none-any.whl (41 kB)\n",
      "Downloading splot-1.1.7-py3-none-any.whl (39 kB)\n",
      "Downloading spopt-0.7.0-py3-none-any.whl (248 kB)\n",
      "Downloading pulp-3.3.0-py3-none-any.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading spreg-1.8.3-py3-none-any.whl (389 kB)\n",
      "Downloading tobler-0.12.1-py3-none-any.whl (28 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading rasterio-1.4.3-cp313-cp313-macosx_14_0_arm64.whl (18.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.7/18.7 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
      "Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl (11 kB)\n",
      "Downloading rasterstats-0.20.0-py3-none-any.whl (17 kB)\n",
      "Downloading fiona-1.10.1-cp313-cp313-macosx_11_0_arm64.whl (14.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading simplejson-3.20.2-cp313-cp313-macosx_11_0_arm64.whl (75 kB)\n",
      "Downloading statsmodels-0.14.5-cp313-cp313-macosx_11_0_arm64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading patsy-1.0.2-py2.py3-none-any.whl (233 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: spint\n",
      "  Building wheel for spint (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for spint: filename=spint-1.0.7-py3-none-any.whl size=31448 sha256=018c5dbd4ce2c7f97bc21ecaf9edcd0cfd99b400eef27df0faed056d2c4a8ea5\n",
      "  Stored in directory: /Users/neethusatravada/Library/Caches/pip/wheels/34/95/4d/89242282d1aebdc0ca0e417988979fd5ff7f1dbc0bee423815\n",
      "Successfully built spint\n",
      "Installing collected packages: mpmath, tqdm, sympy, simplejson, rtree, pulp, patsy, networkx, llvmlite, deprecation, click, affine, numba, cligj, click-plugins, statsmodels, rasterio, quantecon, mapclassify, fiona, rasterstats, libpysal, access, tobler, spreg, segregation, pointpats, momepy, inequality, esda, spglm, spaghetti, giddy, spopt, splot, spint, mgwr, pysal\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38/38\u001b[0m [pysal]m34/38\u001b[0m [splot]ation]\n",
      "\u001b[1A\u001b[2KSuccessfully installed access-1.1.9 affine-2.4.0 click-8.3.0 click-plugins-1.1.1.2 cligj-0.7.2 deprecation-2.1.0 esda-2.8.0 fiona-1.10.1 giddy-2.3.6 inequality-1.1.2 libpysal-4.13.0 llvmlite-0.45.1 mapclassify-2.10.0 mgwr-2.2.1 momepy-0.10.0 mpmath-1.3.0 networkx-3.5 numba-0.62.1 patsy-1.0.2 pointpats-2.5.2 pulp-3.3.0 pysal-25.7 quantecon-0.10.1 rasterio-1.4.3 rasterstats-0.20.0 rtree-1.4.1 segregation-2.5.2 simplejson-3.20.2 spaghetti-1.7.6 spglm-1.1.0 spint-1.0.7 splot-1.1.7 spopt-0.7.0 spreg-1.8.3 statsmodels-0.14.5 sympy-1.14.0 tobler-0.12.1 tqdm-4.67.1\n",
      "✓ All Week 3 libraries installed!\n"
     ]
    }
   ],
   "source": [
    "# Install libraries needed for Week 3\n",
    "!pip install h3 folium scikit-learn matplotlib seaborn geopandas\n",
    "!pip install pysal esda libpysal\n",
    "\n",
    "print(\"✓ All Week 3 libraries installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b501430b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "WEEK 3: GEOSPATIAL ANALYSIS & HOTSPOT DETECTION\n",
      "Team: Kiran Ghumare, Neethu Satravada, Sajitha Mathi\n",
      "======================================================================\n",
      "✓ Loaded 1,971,196 records\n",
      "\n",
      "Available columns:\n",
      "['date_ts', 'crime_type', 'latitude', 'longitude', 'community_area', 'temp_mean', 'precipitation', 'wind_speed', 'year', 'month', 'hour', 'dayofweek', 'season', 'per_capita_income']\n",
      "\n",
      "Converting to Pandas for geospatial analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Converted 1,971,196 records to Pandas\n",
      "\n",
      "✅ Data ready for geospatial analysis!\n",
      "\n",
      "Sample data:\n",
      "              date_ts       crime_type   latitude  longitude  community_area  \\\n",
      "0 2020-08-10 09:45:00          ROBBERY  41.908418 -87.677407            24.0   \n",
      "1 2023-09-06 17:00:00  CRIMINAL DAMAGE  41.886018 -87.633938            32.0   \n",
      "2 2023-09-06 11:00:00            THEFT  41.871835 -87.626151            32.0   \n",
      "3 2019-05-21 08:20:00         BURGLARY  41.856547 -87.695605            29.0   \n",
      "4 2021-07-07 10:30:00      SEX OFFENSE  41.655116 -87.594883            54.0   \n",
      "\n",
      "   temp_mean  year  month  hour  season  \n",
      "0       75.2  2020      8     9  summer  \n",
      "1       75.7  2023      9    17    fall  \n",
      "2       75.7  2023      9    11    fall  \n",
      "3       49.1  2019      5     8  spring  \n",
      "4       75.2  2021      7    10  summer  \n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"WEEK 3: GEOSPATIAL ANALYSIS & HOTSPOT DETECTION\")\n",
    "print(\"Team: Kiran Ghumare, Neethu Satravada, Sajitha Mathi\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load the saved data\n",
    "df_geo = spark.read.parquet(\"../data/processed/integrated_crime_data.parquet\")\n",
    "\n",
    "print(f\"✓ Loaded {df_geo.count():,} records\")\n",
    "\n",
    "# Show column names to confirm\n",
    "print(\"\\nAvailable columns:\")\n",
    "print(df_geo.columns)\n",
    "\n",
    "# Convert to Pandas - use correct column names (already lowercase)\n",
    "print(\"\\nConverting to Pandas for geospatial analysis...\")\n",
    "df_pandas = df_geo.select(\n",
    "    \"date_ts\",\n",
    "    \"crime_type\",      # Already renamed\n",
    "    \"latitude\",        # Already renamed\n",
    "    \"longitude\",       # Already renamed\n",
    "    \"community_area\",  # Already renamed\n",
    "    \"temp_mean\",\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"hour\",\n",
    "    \"season\"\n",
    ").toPandas()\n",
    "\n",
    "print(f\"✓ Converted {len(df_pandas):,} records to Pandas\")\n",
    "print(\"\\n✅ Data ready for geospatial analysis!\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample data:\")\n",
    "print(df_pandas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0000bfb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel '.venv (Python 3.13.3)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import h3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"H3 HEXAGONAL SPATIAL INDEXING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check h3 version\n",
    "print(f\"h3 version: {h3.__version__}\")\n",
    "\n",
    "# Add H3 index to each crime (resolution 8 = ~0.46 km² hexagons)\n",
    "print(\"\\nGenerating H3 hexagonal grid indices...\")\n",
    "\n",
    "# Use the correct function based on h3 version\n",
    "def get_h3_index(lat, lon, resolution=8):\n",
    "    try:\n",
    "        # Try new API (h3 v4+)\n",
    "        return h3.latlng_to_cell(lat, lon, resolution)\n",
    "    except AttributeError:\n",
    "        # Fall back to old API (h3 v3)\n",
    "        return h3.geo_to_h3(lat, lon, resolution)\n",
    "\n",
    "df_pandas['h3_index'] = df_pandas.apply(\n",
    "    lambda row: get_h3_index(row['latitude'], row['longitude'], 8),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"✓ Added H3 indices to {len(df_pandas):,} records\")\n",
    "\n",
    "# Aggregate crimes by hexagon\n",
    "hex_crimes = df_pandas.groupby('h3_index').agg({\n",
    "    'crime_type': 'count',\n",
    "    'latitude': 'mean',\n",
    "    'longitude': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "hex_crimes.columns = ['h3_index', 'crime_count', 'lat_center', 'lon_center']\n",
    "\n",
    "print(f\"✓ Created {len(hex_crimes):,} hexagonal cells\")\n",
    "print(f\"✓ Average crimes per hex: {hex_crimes['crime_count'].mean():.1f}\")\n",
    "\n",
    "# Show top crime hotspot hexagons\n",
    "print(\"\\nTop 10 Crime Hotspot Hexagons:\")\n",
    "print(hex_crimes.nlargest(10, 'crime_count')[['h3_index', 'crime_count', 'lat_center', 'lon_center']])\n",
    "\n",
    "print(\"\\n✅ H3 spatial indexing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d28e5571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h3 in /Users/neethusatravada/Documents/DATABASE/crime_prediction_project/.venv/lib/python3.13/site-packages (4.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U --pre h3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
